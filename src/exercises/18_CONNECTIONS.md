# Exercise 18: RAG Evaluation - CONNECTIONS

## Before You Start (5 minutes)

### What Do You Already Know?

Take a moment to reflect on these questions:

1. **Have you ever measured the quality of a system?**
   - Like checking test scores or user satisfaction.

2. **Do you know about A/B testing?**
   - Comparing two versions to see which is better.

3. **Have you debugged by analyzing patterns in failures?**
   - Finding what's common among things that went wrong.

### Real-World Analogy

Think of RAG Evaluation like **Quality Control in Manufacturing**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            THE QUALITY CONTROL ANALOGY                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Without QC (Flying Blind):                                  â”‚
â”‚  "Our RAG system seems okay... I think?"                    â”‚
â”‚  â†’ No data on actual performance                             â”‚
â”‚  â†’ Problems discovered by angry users                        â”‚
â”‚                                                              â”‚
â”‚  With QC (RAG Evaluation):                                   â”‚
â”‚                                                              â”‚
â”‚  ğŸ“ MEASUREMENT (Metrics):                                   â”‚
â”‚     "Retrieval finds correct doc 78% of the time"           â”‚
â”‚     "Answers are faithful to context 92% of time"           â”‚
â”‚                                                              â”‚
â”‚  ğŸ”¬ TESTING (Evaluation Dataset):                            â”‚
â”‚     Run 500 test questions before each release              â”‚
â”‚     Compare against known correct answers                    â”‚
â”‚                                                              â”‚
â”‚  ğŸ“Š COMPARISON (A/B Testing):                                â”‚
â”‚     "Config A has 72% accuracy, Config B has 78%"           â”‚
â”‚     "Config B is statistically better (p<0.05)"             â”‚
â”‚                                                              â”‚
â”‚  ğŸ” FAILURE ANALYSIS:                                        â”‚
â”‚     "80% of failures are retrieval misses"                  â”‚
â”‚     "Recommendation: Add hybrid search"                      â”‚
â”‚                                                              â”‚
â”‚  Result: Data-driven improvements, not guessing!            â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### What Gets Measured

```
RAG EVALUATION COMPONENTS:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RETRIEVAL EVALUATION                                        â”‚
â”‚  "Did we find the right documents?"                          â”‚
â”‚                                                              â”‚
â”‚  â€¢ Recall@k: Did we find the relevant docs in top k?        â”‚
â”‚  â€¢ Precision@k: Of top k, how many are relevant?            â”‚
â”‚  â€¢ MRR: How high is the first relevant doc ranked?          â”‚
â”‚  â€¢ NDCG: Are relevant docs ranked properly?                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  GENERATION EVALUATION                                       â”‚
â”‚  "Did we produce a good answer?"                             â”‚
â”‚                                                              â”‚
â”‚  â€¢ Faithfulness: Is answer grounded in context?             â”‚
â”‚  â€¢ Answer Relevance: Does it address the question?          â”‚
â”‚  â€¢ Context Relevance: Was retrieved context useful?         â”‚
â”‚  â€¢ Correctness: Is the answer actually correct?             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  END-TO-END EVALUATION                                       â”‚
â”‚  "How good is the whole system?"                             â”‚
â”‚                                                              â”‚
â”‚  â€¢ Accuracy: Correct answers / Total questions              â”‚
â”‚  â€¢ Latency: How fast is the response?                       â”‚
â”‚  â€¢ User Satisfaction: Do users like the answers?            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Prerequisite Checklist

Before starting this exercise, make sure you:

- [ ] Completed Exercises 13-17 (Full RAG stack)
- [ ] Built at least one RAG pipeline
- [ ] Understand basic statistics (mean, p-values)
- [ ] Have seen evaluation concepts in ML

### Connect to Your Goal

**Building the Best RAG System**: You can't improve what you don't measure.

```
The Improvement Cycle:

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   MEASURE    â”‚ â—„â”€â”€â”€ RAG Evaluation
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   ANALYZE    â”‚ â—„â”€â”€â”€ Failure Analysis
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   IMPROVE    â”‚ â—„â”€â”€â”€ Apply techniques from Ex 17
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   VALIDATE   â”‚ â—„â”€â”€â”€ A/B Testing
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Repeat!

This is how you build "the best" RAG system!
```

### Warm-Up Activity

Before coding, think about evaluation:

1. **What questions would you use to test a customer support RAG?**
   - _____________________
   - _____________________
   - _____________________

2. **How would you know if an answer is "faithful" to the context?**
   - _____________________

3. **What would you do if retrieval recall is low?**
   - _____________________

---

**Ready?** Now proceed to `18_rag_evaluation.py` and implement the functions!
